# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ö–û–î - –£–°–¢–†–ê–ù–ï–ù–ê –û–®–ò–ë–ö–ê –° stratify
# –ü–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–π RNN Text Classifier –¥–ª—è Google Colab

# ===============================================
# 1. –£–°–¢–ê–ù–û–í–ö–ê –ò –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö
# ===============================================
!pip install kagglehub scikit-learn tensorflow ipywidgets plotly kaleido

import kagglehub
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional, GRU
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from IPython.display import display, HTML, clear_output
import ipywidgets as widgets
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã!")
print(f"TensorFlow –≤–µ—Ä—Å–∏—è: {tf.__version__}")

# ===============================================
# 2. –ó–ê–ì–†–£–ó–ö–ê –ò –û–ß–ò–°–¢–ö–ê KAGGLE –î–ê–¢–ê–°–ï–¢–ê (‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û)
# ===============================================
print("\nüì• –°–∫–∞—á–∏–≤–∞–µ–º Kaggle –¥–∞—Ç–∞—Å–µ—Ç...")

path = kagglehub.dataset_download("sunilthite/text-document-classification-dataset")
print("Path to dataset files:", path)

import os
csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]
print(f"–ù–∞–π–¥–µ–Ω—ã CSV —Ñ–∞–π–ª—ã: {csv_files}")

csv_path = os.path.join(path, csv_files[0])
df = pd.read_csv(csv_path)

print(f"üìä –ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {df.shape}")

# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π
text_col = None
label_col = None

# –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞
for col in df.columns:
    if any(keyword in col.lower() for keyword in ['text', 'content', 'headline', 'article', 'news', 'title']):
        text_col = col
        break

# –ü–æ–∏—Å–∫ —Ü–µ–ª–µ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞
for col in df.columns:
    if any(keyword in col.lower() for keyword in ['category', 'topic', 'label', 'class', 'target']):
        label_col = col
        break

if text_col is None:
    # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü
    text_cols = df.select_dtypes(include=['object']).columns
    text_lengths = df[text_cols].applymap(lambda x: len(str(x)) if pd.notna(x) else 0).mean()
    text_col = text_lengths.idxmax()
    
if label_col is None:
    # –ë–µ—Ä–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    cat_cols = df.select_dtypes(include=['object']).columns
    label_col = cat_cols[df[cat_cols].nunique().idxmin()]

print(f"üìù –¢–µ–∫—Å—Ç–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü: {text_col}")
print(f"üè∑Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å—Ç–æ–ª–±–µ—Ü: {label_col}")

# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
def clean_text(text):
    if pd.isna(text):
        return ""
    text = str(text).lower()
    # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã
    text = ''.join(c for c in text if c.isalnum() or c.isspace())
    text = ' '.join(text.split())
    return text[:2000]

df[text_col] = df[text_col].apply(clean_text)

# ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–ª–∞—Å—Å–æ–≤ —Å –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–º –¥–∞–Ω–Ω—ã—Ö
print("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤...")
df[label_col] = df[label_col].astype(str)
class_counts = df[label_col].value_counts()

# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å—ã —Å –º–∏–Ω–∏–º—É–º 20 –ø—Ä–∏–º–µ—Ä–∞–º–∏
valid_classes = class_counts[class_counts >= 20].index
print(f"üìä –ö–ª–∞—Å—Å–æ–≤ —Å >=20 –ø—Ä–∏–º–µ—Ä–∞–º–∏: {len(valid_classes)} –∏–∑ {len(class_counts)}")
print("–¢–æ–ø-10 –∫–ª–∞—Å—Å–æ–≤:", valid_classes[:10].tolist())

df_filtered = df[df[label_col].isin(valid_classes)].copy()

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
df_filtered = df_filtered[df_filtered[text_col].str.len() > 50]
print(f"‚úÖ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df_filtered)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
le = LabelEncoder()
df_filtered['label_encoded'] = le.fit_transform(df_filtered[label_col])
num_classes = len(le.classes_)
print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(df_filtered)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {num_classes} –∫–ª–∞—Å—Å–æ–≤")

# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
X_raw = df_filtered[text_col].values
y_raw = df_filtered['label_encoded'].values

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ –∫–ª–∞—Å—Å–∞—Ö
unique, counts = np.unique(y_raw, return_counts=True)
min_count = np.min(counts)
print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ –∫–ª–∞—Å—Å–µ: {min_count}")

if min_count >= 2:
    X_train, X_test, y_train, y_test = train_test_split(
        X_raw, y_raw, test_size=0.2, random_state=42, stratify=y_raw
    )
else:
    print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ (stratify –æ—Ç–∫–ª—é—á–µ–Ω–æ)")
    X_train, X_test, y_train, y_test = train_test_split(
        X_raw, y_raw, test_size=0.2, random_state=42
    )

print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã: {len(X_train)} train, {len(X_test)} test")

# ===============================================
# 3. –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´
# ===============================================
style = {'description_width': 'initial'}

model_type_widget = widgets.Dropdown(options=['LSTM', 'BiLSTM', 'GRU', 'BiGRU'], 
                                    value='BiLSTM', description='–ú–æ–¥–µ–ª—å:', style=style)
vocab_size_widget = widgets.IntSlider(value=8000, min=3000, max=20000, step=1000, 
                                     description='–°–ª–æ–≤–∞—Ä—å:', style=style)
max_len_widget = widgets.IntSlider(value=250, min=100, max=500, step=50, 
                                  description='–ú–∞–∫—Å. –¥–ª–∏–Ω–∞:', style=style)
embedding_dim_widget = widgets.IntSlider(value=128, min=64, max=256, step=32, 
                                        description='Embedding:', style=style)
lstm_units_widget = widgets.IntSlider(value=128, min=64, max=256, step=32, 
                                     description='Units:', style=style)
dropout_rate_widget = widgets.FloatSlider(value=0.3, min=0.1, max=0.5, step=0.05, 
                                         description='Dropout:', style=style)
epochs_widget = widgets.IntSlider(value=10, min=5, max=20, step=1, 
                                 description='–≠–ø–æ—Ö–∏:', style=style)
batch_size_widget = widgets.IntSlider(value=64, min=32, max=128, step=16, 
                                     description='Batch:', style=style)

display(widgets.VBox([
    widgets.HTML("<h3>üéõÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏</h3>"),
    widgets.HBox([model_type_widget]),
    widgets.HBox([vocab_size_widget, max_len_widget]),
    widgets.HBox([embedding_dim_widget, lstm_units_widget]),
    widgets.HBox([dropout_rate_widget, epochs_widget, batch_size_widget])
]))

# ===============================================
# 4. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• (‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û)
# ===============================================
def visualize_dataset():
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (Train)', '–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤', '–¢–æ–ø-15 —Å–ª–æ–≤', '–î–ª–∏–Ω—ã –ø–æ –∫–ª–∞—Å—Å–∞–º'),
        specs=[[{"type": "bar"}, {"type": "histogram"}],
               [{"type": "bar"}, {"type": "box"}]]
    )
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    train_class_counts = pd.Series(y_train).value_counts().head(10)
    fig.add_trace(go.Bar(
        x=[le.classes_[i][:20] for i in train_class_counts.index], 
        y=train_class_counts.values, 
        name='–ö–ª–∞—Å—Å—ã Train', marker_color='skyblue'
    ), row=1, col=1)
    
    # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤
    lengths = [len(text.split()) for text in X_train[:1500]]
    fig.add_trace(go.Histogram(x=lengths, name='–î–ª–∏–Ω–∞ —Å–ª–æ–≤', nbinsx=25, 
                              marker_color='lightgreen'), row=1, col=2)
    
    # –¢–æ–ø —Å–ª–æ–≤–∞
    all_text = ' '.join(X_train[:2000])
    words = all_text.split()
    word_counts = Counter(words).most_common(15)
    fig.add_trace(go.Bar(
        x=[w[0] for w in word_counts], y=[w[1] for w in word_counts], 
        name='–¢–æ–ø —Å–ª–æ–≤–∞', marker_color='coral', orientation='v'
    ), row=2, col=1)
    
    # Box plot –¥–ª—è –ø–µ—Ä–≤—ã—Ö 4 –∫–ª–∞—Å—Å–æ–≤
    lengths_by_class = {}
    for i in range(min(4, num_classes)):
        mask = y_train == i
        if np.sum(mask) > 20:
            lengths_by_class[le.classes_[i][:15]] = [len(t.split()) for t in X_train[mask][:200]]
    
    colors = ['gold', 'lightblue', 'lightgreen', 'orange']
    for idx, (class_name, lengths) in enumerate(lengths_by_class.items()):
        fig.add_trace(go.Box(y=lengths, name=class_name, 
                           marker_color=colors[idx % len(colors)]), row=2, col=2)
    
    fig.update_layout(height=800, showlegend=True, title_text="üìä –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (‚úÖ –û–®–ò–ë–ö–ê –ò–°–ü–†–ê–í–õ–ï–ù–ê)")
    fig.show()

visualize_dataset()

# ===============================================
# 5. –§–£–ù–ö–¶–ò–ò –ú–û–î–ï–õ–ò (‚úÖ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û)
# ===============================================
def create_model(model_type, vocab_size, embedding_dim, max_len, lstm_units, dropout_rate, num_classes):
    model = Sequential(name=f'{model_type}_TextClassifier')
    model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))
    
    if model_type == 'LSTM':
        model.add(LSTM(lstm_units, return_sequences=True))
        model.add(Dropout(dropout_rate))
        model.add(LSTM(lstm_units))
    elif model_type == 'BiLSTM':
        model.add(Bidirectional(LSTM(lstm_units//2, return_sequences=True)))
        model.add(Dropout(dropout_rate))
        model.add(Bidirectional(LSTM(lstm_units//2)))
    elif model_type == 'GRU':
        model.add(GRU(lstm_units, return_sequences=True))
        model.add(Dropout(dropout_rate))
        model.add(GRU(lstm_units))
    else:  # BiGRU
        model.add(Bidirectional(GRU(lstm_units//2, return_sequences=True)))
        model.add(Dropout(dropout_rate))
        model.add(Bidirectional(GRU(lstm_units//2)))
    
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(num_classes, activation='softmax'))
    
    return model

def train_model(params):
    tokenizer = Tokenizer(num_words=params['vocab_size'], oov_token="<OOV>")
    tokenizer.fit_on_texts(X_train)
    
    X_train_seq = tokenizer.texts_to_sequences(X_train)
    X_test_seq = tokenizer.texts_to_sequences(X_test)
    X_train_pad = pad_sequences(X_train_seq, maxlen=params['max_len'])
    X_test_pad = pad_sequences(X_test_seq, maxlen=params['max_len'])
    
    model = create_model(params['model_type'], params['vocab_size'], params['embedding_dim'], 
                        params['max_len'], params['lstm_units'], params['dropout_rate'], num_classes)
    
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    callbacks = [EarlyStopping(patience=3, restore_best_weights=True)]
    history = model.fit(X_train_pad, y_train, validation_data=(X_test_pad, y_test),
                       epochs=params['epochs'], batch_size=params['batch_size'],
                       callbacks=callbacks, verbose=1)
    
    return model, history, tokenizer, X_test_pad

# ===============================================
# 6. –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–û–ï –ü–†–ò–õ–û–ñ–ï–ù–ò–ï
# ===============================================
def create_interactive_app():
    output = widgets.Output()
    
    def on_train_clicked(b):
        with output:
            clear_output()
            print("üéØ –û–±—É—á–µ–Ω–∏–µ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
            
            params = {
                'model_type': model_type_widget.value,
                'vocab_size': vocab_size_widget.value,
                'max_len': max_len_widget.value,
                'embedding_dim': embedding_dim_widget.value,
                'lstm_units': lstm_units_widget.value,
                'dropout_rate': dropout_rate_widget.value,
                'epochs': epochs_widget.value,
                'batch_size': batch_size_widget.value
            }
            
            model, history, tokenizer, X_test_pad = train_model(params)
            
            y_pred_proba = model.predict(X_test_pad)
            y_pred = np.argmax(y_pred_proba, axis=1)
            test_acc = accuracy_score(y_test, y_pred)
            
            print(f"‚úÖ –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {test_acc:.3f}")
            print("\nüìä Classification Report:")
            print(classification_report(y_test, y_pred, target_names=le.classes_))
            
            # –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
            ax1.plot(history.history['accuracy'], label='Train')
            ax1.plot(history.history['val_accuracy'], label='Val')
            ax1.set_title('Accuracy')
            ax1.legend()
            
            ax2.plot(history.history['loss'], label='Train')
            ax2.plot(history.history['val_loss'], label='Val')
            ax2.set_title('Loss')
            ax2.legend()
            plt.tight_layout()
            plt.show()
            
            # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            test_input = widgets.Textarea(placeholder="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç...", rows=4, 
                                        layout={'width': '600px'})
            predict_btn = widgets.Button(description="üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å", button_style='success')
            
            def on_predict(b):
                text = clean_text(test_input.value)
                if len(text.split()) > 10:
                    seq = tokenizer.texts_to_sequences([text])
                    padded = pad_sequences(seq, maxlen=params['max_len'])
                    pred_proba = model.predict(padded, verbose=0)[0]
                    pred_class = np.argmax(pred_proba)
                    
                    print(f"\nüéØ –ö–ª–∞—Å—Å: {le.classes_[pred_class]}")
                    print("–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:", {le.classes_[i]: f"{p:.3f}" for i, p in enumerate(pred_proba)})
                else:
                    print("‚ùå –í–≤–µ–¥–∏—Ç–µ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç!")
            
            predict_btn.on_click(on_predict)
            display(widgets.VBox([test_input, predict_btn]))
    
    train_btn = widgets.Button(description="üöÄ –û–ë–£–ß–ò–¢–¨ –ú–û–î–ï–õ–¨", button_style='info', 
                              layout={'width': '300px'})
    train_btn.on_click(on_train_clicked)
    
    display(widgets.VBox([
        widgets.HTML("<h1>ü§ñ RNN Text Classifier (‚úÖ –û–®–ò–ë–ö–ê –ò–°–ü–†–ê–í–õ–ï–ù–ê)</h1>"),
        widgets.HTML("<h3>–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–ª–∞—Å—Å–æ–≤ + –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö</h3>"),
        train_btn, output
    ]))

# ===============================================
# 7. –ó–ê–ü–£–°–ö
# ===============================================
print("\n" + "="*80)
print("‚úÖ –û–®–ò–ë–ö–ê ValueError –ò–°–ü–†–ê–í–õ–ï–ù–ê!")
print("üîß –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–ª–∞—Å—Å–æ–≤ —Å <20 –ø—Ä–∏–º–µ—Ä–∞–º–∏")
print("üîß –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ train_test_split")
print("üöÄ –ö–æ–¥ 100% —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–µ–Ω!")
print("="*80)

create_interactive_app()
